import pickle
from urllib import parse
import re
import logging

import scrapy
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from newsau.items import AbcDataItem
from newsau.utils import common
from scrapy_redis.spiders import RedisSpider
from newsau.db import orm
from newsau.cache import rcount
from newsau.settings import REDIS_URL
from newsau.settings import NEWS_ACCOUNTS
from redis import Redis

logger = logging.getLogger('abc')

class AbcSpider(RedisSpider):

    name = "abc"
    # be careful primary domain maybe contain other domain to store the image src, so you must remember allowed the domains.
    # allowed_domains = ["abc.net.au", "live-production.wcms.abc-cdn.net.au"]

    # when we use scrapy_redis, so this start_urls don't need it.
    # start_urls = ["https://www.abc.net.au/news/justin"]
    total_urls = 0
    redis_key = "abcspider:start_urls"
    homepage = "https://www.abc.net.au/news/justin"
    seen = set()
    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36',
    }
    # Number of url to fetch from redis on each attempt
    # Update this as needed - this will be 16 by default (like the concurrency default)
    redis_batch_size = 1

    def __init__(self, *args, **kwargs):
        # Dynamically define the allowed domains list.
        # Be careful primary domain maybe contain other domain to store the image src, so you must remember allowed the domains.
        domain = kwargs.pop("abc.net.au", "live-production.wcms.abc-cdn.net.au")
        self.allowed_domains = filter(None, domain.split(","))

        self.domain = "https://www.abc.net.au/"

        self.count = rcount.RedisCounter(self.name, REDIS_URL)
        if self.count.get_value() is None or self.count.get_value() <= 0:
            self.count.set_value(NEWS_ACCOUNTS[self.name]["count_everyday"])
        logger.info(f'current count_everyday is:{self.count.get_value()}')

        super().__init__(*args, **kwargs)
        self.r = Redis.from_url(REDIS_URL, decode_responses=True)
        
        # 从Redis加载已见过的URL
        self.seen_key = f"{self.name}:seen_urls"
        
        # 检查seen数据量，如果超过1万条则清理
        seen_count = self.r.scard(self.seen_key)
        if seen_count > 10000:
            logger.warning(f'Seen URLs count ({seen_count}) exceeds 10000, clearing Redis seen data')
            self.r.delete(self.seen_key)
            self.seen = set()
        else:
            self.seen = set(self.r.smembers(self.seen_key))
            
        logger.info(f'Loaded {len(self.seen)} seen URLs from Redis')

    def add_to_seen(self, url):
        """将URL添加到已见过的集合中，同时保存到Redis"""
        self.seen.add(url)
        self.r.sadd(self.seen_key, url)

    def clear_seen(self):
        """清理已见过的URL集合（可选，用于重置）"""
        self.r.delete(self.seen_key)
        self.seen.clear()
        logger.info('Cleared seen URLs from Redis')

    def start_requests(self):
        for url in self.r.lrange(self.redis_key, 0, -1):
            logger.info(f'start_requests url:{url}')
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        url = response.url.rstrip('/')
        
        if url == self.homepage:
            logger.info("Detected homepage — extracting first valid article link")
            # 获取所有文章链接
            post_nodes = response.xpath('//div[@data-component="PaginationList"]//ul/li')
            
            for post_node in post_nodes:
                post_url = post_node.css('h3 a::attr(href)').extract_first("").strip()
                
                if post_url:
                    # 跳过已见过的链接
                    if post_url in self.seen:
                        logger.info(f'URL already seen: {post_url}')
                        continue
                        
                    abs_url = urljoin(self.domain, post_url)
                    logger.info(f'Found article URL: {abs_url}')
                    
                    # 跳过已爬取的内容
                    if orm.query_object_id(self.name, common.get_md5(abs_url)):
                        logger.info(f'URL already processed: {abs_url}')
                        self.add_to_seen(post_url)  # 标记为已见过
                        continue
                    
                    # 检查是否包含有效日期
                    if not common.contains_valid_date(abs_url):
                        logger.info(f'URL does not contain valid date: {abs_url}')
                        self.add_to_seen(post_url)  # 标记为已见过
                        continue
                    
                    # 检查是否超过每日限制
                    if orm.check_if_exceed_num(self.name, self.count.get_value()):
                        logger.info('Exceeded daily limit, stopping crawl.')
                        return
                    
                    # 记录已处理链接，生成请求并退出
                    self.add_to_seen(post_url)
                    yield scrapy.Request(
                        abs_url,
                        callback=self.detail_parse,
                        dont_filter=True,
                        meta={"is_priority": True}
                    )
                    return  # 抓取首条链接后立即退出
            
            logger.info("No valid new link found on homepage")
        else:
            # 直接处理文章页面
            yield scrapy.Request(
                response.url,
                callback=self.detail_parse,
                dont_filter=True,
                meta={"is_priority": True}
            )

    def detail_parse(self, response):
        is_priority = response.meta.get('is_priority', False)

        if not is_priority and orm.check_if_exceed_num(self.name, self.count.get_value()):
            logger.info('Exceeded daily limit, skipping this URL.')
            return

        logger.info(f'Processing detail page: {response.url}')

        abc_item = AbcDataItem()
        abc_item["name"] = self.name
        abc_item["priority"] = is_priority

        post_title = response.xpath('//*[@id="content"]/article//header//h1/text()').extract_first("").strip()
        if not post_title:
            post_title = response.css('div[data-component="ArticleWeb"] h1::text').extract_first("").strip()

        if not post_title:
            logger.warning(f'No title found for URL: {response.url}')
            # 把首页重新推入 Redis, 重新选择一个链接
            payload = '{"url": "' + self.homepage + '", "meta": {"schedule_num":1}}'
            self.r.lpush(self.redis_key, payload)
            return

        post_topic = response.xpath('//*[@id="content"]/article//header//ul/li//p/text()').extract_first("").strip()
        if post_topic == '':
            post_topic = response.css('div[data-component="ArticleWeb"] li a[data-component="SubjectTag"] p::text').extract_first("").strip()
        if post_topic == '':
            post_topic = response.xpath('//*[@id="content"]/article/div//a[@data-component="InfoSourceTag"]/p/text()').extract_first("").strip()

        post_header = response.xpath('//*[@id="content"]/article/div/div[1]/div[1]/div[3]').extract_first("").strip()

        post_content = response.xpath('//*[@id="body"]//div[contains(@class,ArticleRender)]/text()').extract_first("").strip()
        if not post_content:
            post_content = response.xpath('//*[@id="content"]/article/div/div[2]/div/div[1]').extract_first("").strip()

        if not post_content:
            logger.warning(f'No content found for URL: {response.url}')
            # 把首页重新推入 Redis, 重新选择一个链接
            payload = '{"url": "' + self.homepage + '", "meta": {"schedule_num":1}}'
            self.r.lpush(self.redis_key, payload)
            return

        # data-component = "Timestamp"
        # datetime = "2025-02-10T04:55:05.000Z"
        # //*[@id="content"]/article/div/div[1]/div[1]/div[2]/div/time[1]
        post_time = response.xpath('//time[@data-component="Timestamp"]/@datetime').extract_first("").strip()
        abc_item["post_date"] = common.convert_to_datetime(post_time)

        if post_header != '':
            post_content = post_header + post_content

        abc_item["origin_title"] = post_title
        abc_item["topic"] = post_topic
        abc_item["url"] = response.url
        abc_item["url_object_id"] = common.get_md5(abc_item["url"])

        if orm.query_object_id(self.name, abc_item["url_object_id"]):
            logger.info(f'URL already processed: {abc_item["url"]}')
            # 把首页重新推入 Redis, 重新选择一个链接
            payload = '{"url": "' + self.homepage + '", "meta": {"schedule_num":1}}'
            self.r.lpush(self.redis_key, payload)
            return

        abc_item["front_image_url"] = []

        # process the content
        # find all the images src in the post_content
        # and store these images src
        # and replace the domain of the src in post_content
        soup = BeautifulSoup(post_content,"html.parser")
        for img in soup.findAll('img'):
            abc_item["front_image_url"].append(img['src']) # append origin url to download
            img['src'] = common.get_finished_image_url(self.name, abc_item["url_object_id"], img['src']) # replace our website image url from cdn

        # find all a label
        for a in soup.find_all('a'):
            # replace all a label with its text
            a.replace_with(a.text)

        # trim div data-component="EmbedBlock"
        for div in soup.find_all('div', {"data-component":"EmbedBlock"}):
            div.decompose()

        # trim span data-component="Loading" data-print="inline-media"
        for span in soup.find_all('span', {"data-component":"Loading"}):
            span.decompose()

        post_content = str(soup)
        abc_item["origin_content"] = post_content

        if abc_item["url"] != "" and abc_item["origin_title"] != "" and abc_item["origin_content"] != "":
            logger.info(f'Processing item: {abc_item}')
            # yield abc_item
        else:
            logger.warning(f'Invalid item, missing data: {abc_item}')
            # 把首页重新推入 Redis, 重新选择一个链接
            payload = '{"url": "' + self.homepage + '", "meta": {"schedule_num":1}}'
            self.r.lpush(self.redis_key, payload)